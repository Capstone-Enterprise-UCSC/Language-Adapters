{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e1572d-e876-433f-a076-07e0e138f02a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers\n",
      "  Cloning https://github.com/akufeldt/adapter-transformers.git (to revision debug) to /tmp/pip-install-_fdlrr0g/adapter-transformers_472992024fa74a4d8b3f0dbe76916339\n",
      "  Running command git clone --quiet https://github.com/akufeldt/adapter-transformers.git /tmp/pip-install-_fdlrr0g/adapter-transformers_472992024fa74a4d8b3f0dbe76916339\n",
      "  Resolved https://github.com/akufeldt/adapter-transformers.git to commit d07a91354c916c5ab9ec630065584e9d1bd3d666\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers)\n",
      "  Using cached tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from requests->adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from requests->adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from requests->adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from requests->adapter-transformers@ git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.34.0 requires tokenizers<0.15,>=0.14, but you have tokenizers 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"adapter-transformers@git+https://github.com/akufeldt/adapter-transformers.git@debug#egg=adapter-transformers&subdirectory=adapter-transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9f95e2-fb0e-487d-8e7e-4a18f3c97c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/76/ee/7e35fb46c728989357e6ccb96df64c4364601cfbfdd6c25ccc872e6c16a0/tokenizers-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mkar/miniconda3/envs/capstone/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Using cached tokenizers-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "adapter-transformers 3.2.1 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3551c529-fcb2-458d-8e5c-4544876eba0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Softmax\n",
    "\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_metric, load_from_disk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from transformers import PreTrainedModel, TrainingArguments\n",
    "from transformers.adapters import AdapterTrainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a7310e-77b6-4eb1-87ac-9e6afd4bcbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef787507-63c0-45ca-ba66-d56f4141dc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_numpy_rng = np.random.default_rng(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1631e252-26e7-4ec8-ac5b-efdd94a678fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7895b82a-ddd2-481b-8932-0907b73462a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ea527",
   "metadata": {},
   "source": [
    "# Load in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d93d087-e26d-4036-be92-8fe797856c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'm2m100_418M'\n",
    "experiment = 'ha-en-lang-adapter-1'\n",
    "dataset_name = 'Data/en-ha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcb15db-3e10-4c2d-a788-9f3d755fa5f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(f\"facebook/{model_name}\")\n",
    "# model = torch.nn.DataParallel(model, device_ids=[2, 3, 4])\n",
    "model = model.to(device)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(f\"facebook/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e3b53",
   "metadata": {},
   "source": [
    "# Create adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a979bd3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_adapter() got an unexpected keyword argument 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m enc_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpfeiffer[output_adapter=False,monolingual_enc_adapter=True]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m dec_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpfeiffer[output_adapter=False,monolingual_dec_adapter=True]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menc_en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd_adapter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdec_ha\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39mdec_config)\n",
      "\u001b[0;31mTypeError\u001b[0m: add_adapter() got an unexpected keyword argument 'config'"
     ]
    }
   ],
   "source": [
    "# NOTE : also try with original_ln_after=False, which is more theoretically correct but may not result in best performance\n",
    "enc_config = \"pfeiffer[output_adapter=False,monolingual_enc_adapter=True]\"\n",
    "dec_config = \"pfeiffer[output_adapter=False,monolingual_dec_adapter=True]\"\n",
    "\n",
<<<<<<< HEAD
    "model.add_adapter(\"enc_ha\", config=enc_config)\n",
    "model.add_adapter(\"dec_en\", config=dec_config)"
=======
    "# Add lang adapters\n",
    "model.add_adapter(\"enc_en\", config=enc_config)\n",
    "model.add_adapter(\"dec_ha\", config=dec_config)"
>>>>>>> 98e0bf67522c98f172e0a520fc12de6ecddca796
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f70631-9a45-4400-a064-c55d61bd4a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.add_adapter(\"enc_indo_euro\", config=enc_config)\\nmodel.add_adapter(\"dec_afro_asiatic\", config=dec_config'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Add lang adapters\n",
    "model.add_adapter(\"enc_indo_euro\", config=enc_config)\n",
    "model.add_adapter(\"dec_afro_asiatic\", config=dec_config\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da91567",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f8621a-91a7-430f-abb8-b24099ba7ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_lang = 'ha'\n",
    "tgt_lang = 'en'\n",
    "tokenizer.src_lang = \"ha\"\n",
    "tokenizer.tgt_lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47843c49-de6f-4f0e-b972-33aea21d6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({'train':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_train.csv')),\n",
    "                        'validation':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_dev.csv')),\n",
    "                        'test':Dataset.from_pandas(pd.read_csv(f'{dataset_name}/cleaned_test.csv'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fab027a2-400a-4961-90b4-da9b28449308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ha'],\n",
       "        num_rows: 9818\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'ha'],\n",
       "        num_rows: 1113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e09ae18-7223-405e-a273-4b6b39725f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [example for example in examples[src_lang]]\n",
    "    targets = [example for example in examples[tgt_lang]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b0dc3d-64be-41f0-8527-17fcbb3cc657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2269eff71f4a9280528226958c7970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502e95c76e274712bf882f7a780f8178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee83e5f-9065-43f5-bc23-624fa8635ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9818\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc0a643",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ad4b70-d1f7-4407-b135-9ee631f83f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7816c388-e691-4c08-874f-14fc97dac435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    preds = np.argmax(pred_ids, axis=-1)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Removeme\n",
    "    import warnings\n",
    "    warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
    "    warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
    "\n",
    "    result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c1dc421-b458-4edd-94de-c6b18fdffe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a486406-df4c-4e54-93a5-ba859da8c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers.adapters.composition as ac\n",
    "\n",
    "# Activate lang adapters\n",
    "model.train_adapter_pair(ac.Pair(\"enc_en\",\"dec_ha\"))\n",
    "\n",
    "\"\"\"\n",
    "# Activate family adapters\n",
    "encoder_adapters = ac.Stack(\"enc_indo_euro\",\"enc_en\")\n",
    "decoder_adapters = ac.Stack(\"dec_afro_asiatic\",\"dec_ha\")\n",
    "model.train_adapter_pair(ac.Pair(encoder_adapters,decoder_adapters))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a61a2b25-0b90-4074-81c2-f80b6f0e26c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"./lang_adapters/{experiment}/model\",\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    warmup_steps=1000,\n",
    "    # lr_scheduler_type='constant',\n",
    "    # gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    # gradient_checkpointing=True,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=5,\n",
    "    # eval_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #optimizers=(optimizer, lr_scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2783c81-d3a5-48b7-a18d-74bc3a01ce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(getattr(model.base_model, \"model_frozen\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a361ae7-3f1f-413a-bda9-1e6fe6ee4899",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akufeldt/miniconda3/envs/nlp_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9818\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3080\n",
      "  Number of trainable parameters = 4757760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='309' max='3080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 309/3080 09:22 < 1:24:34, 0.55 it/s, Epoch 2/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.915100</td>\n",
       "      <td>6.831612</td>\n",
       "      <td>0.580800</td>\n",
       "      <td>256.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1113\n",
      "  Batch size = 16\n",
      "/tmp/ipykernel_5822/4083629571.py:25: UserWarning: preds: A wannan,' cikinashinun da'in da, baibai bas basyan basiki ba kuma ba cikin daai basij. kuma ba da'ari da kuma da ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ ਅੰਮ੍ਰਿਤਸਰ\n",
      ")\n",
      "  warnings.warn(f\"preds: {decoded_preds[0]}\\n)\")\n",
      "/tmp/ipykernel_5822/4083629571.py:26: UserWarning: labels: A yau ma a makarantun hauzar mu akwai dalibai da manyan malamai da suke a matsayin dakarun Basiji, kuma suna alfahari da hakan.\n",
      ")\n",
      "  warnings.warn(f\"labels: {decoded_labels[0]}\\n)\")\n",
      "Saving model checkpoint to ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154\n",
      "Configuration saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/enc_en/adapter_config.json\n",
      "Module weights saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/enc_en/pytorch_adapter.bin\n",
      "Configuration saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/enc_en/head_config.json\n",
      "Module weights saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/enc_en/pytorch_model_head.bin\n",
      "Configuration saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/dec_ha/adapter_config.json\n",
      "Module weights saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/dec_ha/pytorch_adapter.bin\n",
      "Configuration saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/dec_ha/head_config.json\n",
      "Module weights saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/dec_ha/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/tokenizer_config.json\n",
      "Special tokens file saved in ./lang_adapters/en-ha-lang-adapter-1/model/checkpoint-154/special_tokens_map.json\n",
      "Deleting older checkpoint [lang_adapters/en-ha-lang-adapter-1/model/checkpoint-1232] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1113\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27388ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapters\n",
    "if not os.path.exists(f'./lang_adapters/{experiment}'):\n",
    "    os.mkdir(f'./lang_adapters/{experiment}')\n",
    "    \n",
    "model.save_adapter(f\"./lang_adapters/{experiment}/encoder_english\", \"enc_en\")\n",
    "model.save_adapter(f\"./lang_adapters/{experiment}/decoder_hausa\", \"dec_ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecfaee-465c-4a3c-ad11-82837d41869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = MyCustomWeightsLoader(model)\n",
    "loader.save(f\"./lang_adapters/{experiment}/encoder_english\", \"enc_en\")\n",
    "loader.save(f\"./lang_adapters/{experiment}/decoder_hausa\", \"dec_ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928793b-3fa2-45ab-aec7-3b0634a32869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'ha'\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"ha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4e6d2-f884-4a9c-a4ff-c1fa70b55da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_outputs = trainer.predict(tokenized_dataset['test'], forced_bos_token_id=tokenizer.get_lang_id(\"ha\"))\n",
    "test_output_texts = tokenizer.batch_decode(torch.LongTensor(test_outputs.predictions), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024e8a5-7972-4c42-a4e1-dbe23c20b7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_outputs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdf73f-64e8-40de-b6d8-2ab50b234f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f'./lang_adapters/{experiment}'):\n",
    "    os.mkdir(f'./lang_adapters/{experiment}')\n",
    "\n",
    "with open(f'./lang_adapters/{experiment}/predictions', 'w') as fp:\n",
    "    for translation in test_output_texts:\n",
    "        fp.write(translation + '\\n')\n",
    "fp.close()\n",
    "\n",
    "json.dump(test_outputs.metrics, open(f'./lang_adapters/{experiment}/metrics', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee711a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in adapters (for future reference)\n",
    "\n",
    "#model.load_adapter(f\"/lang_adapters/{experiment}/encoder_english\", config=enc_config)\n",
    "#model.load_adapter(f\"/lang_adapters/{experiment}/decoder_hausa\", config=dec_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
